{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca276",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter using NLTK in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b192f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73740c3c",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"twitter_sentiment.csv\")\n",
    "print(df.head())\n",
    "print(df[\"sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d45ac2",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_tweet\"] = df[\"tweet\"].apply(clean_tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e535e",
   "metadata": {},
   "source": [
    "## EDA Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f51537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"tweet_length\"] = df[\"clean_tweet\"].apply(len)\n",
    "df[\"word_count\"] = df[\"clean_tweet\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "sns.countplot(x=\"sentiment\", data=df)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[\"tweet_length\"], bins=30)\n",
    "plt.title(\"Tweet Length Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7e786",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba017e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"clean_tweet\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68b8f2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Naive Bayes\n",
    "nb_model = BernoulliNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "\n",
    "# SVM\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b7b43",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf672c7",
   "metadata": {},
   "source": [
    "## Predict New Tweet Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d003fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentiment(text):\n",
    "    text = clean_tweet(text)\n",
    "    vector = vectorizer.transform([text])\n",
    "    return lr_model.predict(vector)[0]\n",
    "\n",
    "print(predict_sentiment(\"I love this product!\"))\n",
    "print(predict_sentiment(\"This is the worst experience ever\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
